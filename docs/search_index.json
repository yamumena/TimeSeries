[["index.html", "Series de Tiempo Actividad 1 1 Selección de la base de datos 1.1 Selección de la Variable Temperatura", " Series de Tiempo Actividad 1 Grupo 5: Yamuna Mena Ramirez Angelica Forero Artunduaga Leidy Conde Chavarro 2024-05-19 1 Selección de la base de datos Se ha elegido la base de datos “Daily Climate time series data”, disponible al público y desarrollada por la Universidad PES. Se Puede acceder a ella a través del siguiente enlace:: https://www.kaggle.com/datasets/sumanthvrao/daily-climate-time-series-data/data. Esta base de datos contiene registros meteorológicos recopilados en la ciudad de Delhi durante un período de 4 años (de 2013 a 2017). Los atributos incluyen la fecha, temperatura media, humedad, velocidad del viento y presión atmosférica, con más de 1400 registros disponibles. 1.1 Selección de la Variable Temperatura Se elige la serie de tiempo de la temperatura como variable para el proceso de pronóstico. Esta decisión se basa en la relevancia global de la temperatura y su influencia significativa en múltiples áreas. En el contexto práctico, el pronostico de la temperatura es beneficioso porque: El pronostico de la temperatura facilita la anticipación de condiciones climáticas extremas como olas de calor, tormentas y fenómenos como El Niño y La Niña. Esto permite adoptar medidas de protección para garantizar la seguridad de las personas y sus propiedades. La temperatura es de gran importancia en sectores como la agricultura, donde afecta los ciclos de cultivo y la productividad. Asimismo, en el ámbito energético, las variaciones de temperatura influyen directamente en la demanda de energía y la eficiencia de fuentes renovables como la solar y eólica. El pronóstico preciso de la temperatura es importante para la economía, ya que permite a las empresas y sectores económicos anticipar y mitigar riesgos financieros asociados con eventos climáticos. Al contar con información anticipada sobre las variaciones de temperatura, las empresas pueden ajustar su producción, inventario y estrategias de mercado, evitando pérdidas económicas por condiciones climáticas adversas. La comprensión de la temperatura a lo largo del tiempo contribuye al estudio del clima y el cambio climático. Esto apoya la investigación científica en áreas como meteorología y climatología, proporcionando datos valiosos para entender mejor nuestro entorno y los desafíos que enfrentamos. "],["análisis-exploratorio-de-datos-eda-promedio-móvil-rezagos-y-estacionalidad.html", "2 Análisis Exploratorio de Datos (EDA): Promedio Móvil, Rezagos y Estacionalidad 2.1 Carga de datos 2.2 Resumen general de la serie temporal 2.3 Visualización de la serie temporal 2.4 Media movil 2.5 Rezago 2.6 Estacionalidad", " 2 Análisis Exploratorio de Datos (EDA): Promedio Móvil, Rezagos y Estacionalidad En esta sección, proporcionaremos una vista general de los datos utilizando algunas técnicas y pruebas, que incluyen la implementación del promedio móvil, análisis de rezagos y detección de estacionalidad. Estas herramientas ayudan a comprender mejor el comportamiento general de la serie temporal de la temperatura. 2.1 Carga de datos Se carga toda la base de datos y se elige solamente la variable temperatura. Con un total de 1.576 registros. # Cargar datos data1 &lt;- read.csv(&quot;C:/Maestria Ciencia de Datos/TStempe/DailyDelhiClimateTrain.csv&quot;, sep = &quot;,&quot;, dec = &quot;.&quot;) data2 &lt;- read.csv(&quot;C:/Maestria Ciencia de Datos/TStempe/DailyDelhiClimateTest.csv&quot;, sep = &quot;,&quot;, dec = &quot;.&quot;) # Unir datos data &lt;- rbind(data1, data2) # Convertir los datos a serie de tiempo ts_data &lt;- ts(data,start = c(2013, 1), end = c(2017, 4), frequency=365) # Escoger solo Temperatura media ts_data &lt;- ts_data[, &quot;meantemp&quot;] 2.2 Resumen general de la serie temporal # Información general str(ts_data) ## Time-Series [1:1464] from 2013 to 2017: 10 7.4 7.17 8.67 6 ... # Valores nulos sum(is.na(ts_data)) ## [1] 0 # Resumen estadistico summary(ts_data) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 6.00 18.81 27.71 25.48 31.29 38.71 A partir de la información proporcionada en el sitio de descarga se sabe que cada registro de la Temperatura media es promediada a partir de múltiples intervalos de 3 horas en un día. Hay 1576 registros No hay valores nulos Hay resgistros desde el 01/01/2013 hasta el 24/04/2017 El valor minimo registro es 6 y el maximo es 38.71 2.3 Visualización de la serie temporal plot(ts_data, main = &quot;Temperatura media&quot;, xlab = &quot;Año&quot;) A partir de la grafica anterior, se aprecia un patrón periódico anual, con mínimos oscilando entre 5 y 15, y máximos cercanos a 35. 2.4 Media movil La media movil se aplica a toda la serie para suavizarla y obtener una mejor comprensión de su comportamiento. Este proceso ayuda a resaltar tendencias y a identificar mejor cualquier patrón estacional que pueda estar presente en la serie de temperatura. library(zoo) ventana &lt;- 50 serie_suavizada &lt;- rollmean(ts_data, k = ventana, align = &quot;center&quot;, fill = NA) # Grafica la serie original y suavizada plot(ts_data, type = &quot;l&quot;, col = &quot;blue&quot;, lwd = 2, main = &quot;Serie Temporal con Media Móvil&quot;, xlab = &quot;Tiempo&quot;, ylab = &quot;Valor Original&quot;) lines(serie_suavizada, col = &quot;red&quot;, lwd = 2) legend(&quot;topright&quot;, legend = c(&quot;Original&quot;, &quot;Suavizada&quot;), col = c(&quot;blue&quot;, &quot;red&quot;), lwd = 1, cex = 0.6) En la gráfica anterior, al observar la serie suavizada se ve representada de manera más clara la estacionalidad de esta variable en el tiempo , confirmando la observación inicial sobre su periodicidad anual. Además, se aprecia que los mínimos son mas grandes y los máximos son menos prominentes que la serie original. Se eligió una ventana de 50 después de experimentar con diferentes tamaños de ventana, buscando aquella que mostrara de manera más clara el comportamiento de la serie con menos iteraciones o variabilidad de la serie original, con el objetivo de obtener una mejor representación de la serie . 2.5 Rezago El análisis de rezago es una técnica comúnmente empleada en el análisis de series de tiempo para examinar la correlación entre una serie temporal y sus valores rezagados en el tiempo. Su objetivo principal es identificar patrones temporales, evaluar la dependencia temporal, detectar estacionalidad y facilitar la selección y validación de modelos adecuados. 2.5.1 Calcular la autocorrelación Se emplea la función ACF para calcular la autocorrelación con un rezago de 365 días, lo que permite analizar un período completo de un año. En el gráfico, el rezago se representará en el eje x en una escala de 0 a 1. Para interpretarlo en días, se multiplica el valor del rezago por 365. Paralelamente, la autocorrelación se visualiza en el eje y, abarcando un rango de -1 a 1. acf_result &lt;- acf(ts_data, lag.max = 365, main = iconv(&quot;Autocorrelación de Temperatura&quot;, to = &quot;UTF-8&quot;)) Se observa que a medida de que nos alejamos del primer valor, las correlaciones disminuyen gradualmente hasta que eventualmente se vuelven negativas. El pico más alto negativo se registra en el rezago 196 (equivalente a 195 días atrás), con una correlación de -0.5927, lo que sugiere una reversión de tendencia a medida que los rezagos aumentan. Sin embargo, la correlación vuelve a disminuir hasta el rezago 288 (287 días atrás), donde se observa un nuevo incremento en la correlación. Este patrón sugiere la presencia de un ciclo estacional en los datos de temperatura. Dado que se estan analizando datos de temperatura diaria, esto confirmaria el ciclo estacional anual que se ha venido observando. Es importante destacar que las correlaciones positivas indican una tendencia a que las temperaturas altas o bajas persistan en el tiempo. Por ejemplo, si hoy es un día cálido, es probable que mañana también lo sea. En contraste, cuando las correlaciones son negativas, sugieren una reversión en la tendencia. Es decir, si hoy es más cálido de lo normal, es probable que mañana sea más frío de lo normal, y viceversa. En este caso, esta reversión negativa se observa entre el rezago 89 (88 días atrás) y el 279 (278 días atrás). 2.6 Estacionalidad Hasta el momento, hemos observado ciclos distintivos en la serie de tiempo proporcionada, la cual se enfoca en la temperatura. Tales ciclos son típicos en series temporales asociadas al clima. Para determinar la presencia de estacionalidad en una serie de tiempo, se puede utilizar: Análisis gráfico: Al representar gráficamente la serie de tiempo, es posible identificar patrones recurrentes que sugieren estacionalidad, como la presencia de picos y valles que se repiten en intervalos regulares. Prueba de estacionalidad de Mann-Kendall: Esta prueba, que es no paramétrica, evalúa la presencia de tendencias monótonas a lo largo del tiempo en la serie de tiempo. Una serie que exhibe estacionalidad a menudo carecerá de una tendencia monótona clara. Análisis de autocorrelación: Al calcular la autocorrelación de la serie de tiempo en diferentes rezagos, se puede determinar si existen autocorrelaciones significativas en los rezagos que están relacionados con la longitud de la estacionalidad supuesta. Como pudimos observar en el análisis de rezago, existen autocorrelaciones significativas en algunos rezagos que podrían indicarnos una estacionalidad anual.Como observamos en el análisis de rezagos, hemos identificado autocorrelaciones significativas en ciertos rezagos. Estas observaciones sugieren la posibilidad de una estacionalidad anual en los datos. Pruebas de estacionalidad de Box-Pierce o Ljung-Box: Estas pruebas se utiliza para evaluar si hay autocorrelaciones significativas en la serie después de eliminar cualquier componente de tendencia y estacionalidad. Si hay autocorrelaciones significativas en los rezagos que corresponden a la estacionalidad, esto puede indicar la presencia de estacionalidad en la serie. Descomposición estacional: Mediante técnicas de descomposición de series de tiempo, es posible separar los componentes de tendencia, estacionalidad y aleatoriedad. Si la componente estacional es notable en comparación con los otros componentes, esto puede indicar la presencia de estacionalidad en la serie. 2.6.1 Prueba de kendall # Prueba de Kendall test_kendall &lt;- MannKendall(ts_data) print(test_kendall) ## tau = 0.0755, 2-sided pvalue =1.5497e-05 El resultado de la prueba muestra un valor de tau de 0.146 y un p-valor menor a 0.05. Esto indica que hay evidencia significativa para rechazar la hipótesis nula de ausencia de estacionalidad en la serie de tiempo. Es decir, se sugiere la presencia de una tendencia monotónica en los datos, lo que podría ser indicativo de estacionalidad. Retomando el análisis grafico que se ha venido haciendo y de rezago, se puede comprobar visualmente que la serie presenta un comportamiento estacional cada año. "],["análisis-exploratorio-de-datos-eda-descomposición-estacionariedad-y-la-diferenciación.html", "3 Análisis Exploratorio de Datos (EDA): Descomposición, Estacionariedad y la Diferenciación 3.1 Descomposición 3.2 Estacionariedad 3.3 Diferenciación", " 3 Análisis Exploratorio de Datos (EDA): Descomposición, Estacionariedad y la Diferenciación En esta sección, proporcionaremos una vista general de los datos utilizando la evaluaciòn de la descomposición, estacionariedad y la diferenciación. 3.1 Descomposición La descomposición de series de tiempo es una técnica esencial en el análisis de datos temporales, permitiendo desglosar una serie en sus componentes principales: tendencia, estacionalidad y ruido. Al aplicar esta técnica a la serie temporal de temperatura, se revela el comportamiento estacional observado previamente, así como la tendencia y la componente de ruido de manera detallada. Esta última representa la variabilidad aleatoria o irregular que no puede atribuirse a la tendencia ni a la estacionalidad previamente identificadas. descomposicion &lt;- decompose(ts_data) plot(descomposicion) En las graficas anteriores se observa lo siqguiente: La tendencia de la serie temporal indica un aumento gradual en las temperaturas a lo largo del tiempo, aunque inicialmente este incremento puede parecer moderado o bastante lento. Sin embargo, en los últimos dos años se ha observado un incremento más pronunciado, sugiriendo que el calentamiento global podría estar acelerándose. y esperando ese posible comportamiento al aumento en proximos años. A pesar de que hay una tendencia lenta o moderada hacia el aumento de la temperatura al pasar los años, la estacionalidad parece mantenerse constante. En cuanto al componente de ruido, se evidencian fluctuaciones que no pueden ser explicadas por la tendencia ni por la estacionalidad de la serie temporal de temperatura. A pesar de la presencia de estas fluctuaciones no sistemáticas, aún se percibe claramente la presencia de la tendencia y la estacionalidad en la serie de tiempo. 3.2 Estacionariedad Para que una serie de tiempo sea considerada estacionaria, es importante que tanto la media como la varianza se mantengan constantes a lo largo del tiempo. Es decir, la serie no debe exhibir tendencias a largo plazo ni variaciones sistemáticas en su comportamiento a medida que transcurre el tiempo. Para lograr la estacionariedad, se deben cumplir los siguientes requisitos: Media Constante: La media de la serie debe permanecer invariable a lo largo del tiempo, lo que indica la ausencia de tendencias significativas hacia arriba o hacia abajo en la serie. Varianza Constante: Asimismo, la varianza de la serie debe mantenerse constante a lo largo del tiempo. Esto implica que la amplitud de las fluctuaciones alrededor de la media no experimenta cambios significativos a medida que progresa el tiempo. Autocovarianza Constante: Es fundamental que la covarianza entre dos puntos de datos separados por un cierto intervalo de tiempo permanezca constante. En otras palabras, la relación entre los valores de la serie en diferentes momentos no debe cambiar a medida que avanza el tiempo. Estos principios son fundamentales para garantizar la estacionariedad de una serie de tiempo, aunque en la práctica se pueden utilizar técnicas adicionales, como la diferenciación, para lograr este objetivo. 3.2.1 Verificar si la serie de temperatura es estacionaria Para evaluar la estacionariedad de una serie de tiempo, podemos llevar a cabo varios métodos: Realizar un análisis visual aplicando los principios previamente mencionados para detectar si la serie presenta una media y varianza constantes a lo largo del tiempo. Aplicar la prueba de Dickey-Fuller, la cual busca determinar si una serie de tiempo posee una raíz unitaria. La presencia de esta raíz implica una tendencia determinística en la serie, lo cual contradice los principios de estacionariedad mencionados anteriormente. La hipótesis nula de esta prueba es que la serie de tiempo tiene una raíz unitaria, lo que indica no estacionariedad. Si el valor p asociado con la prueba es menor que un nivel de significancia predefinido (generalmente 0.05), se rechaza la hipótesis nula y se concluye que la serie de tiempo es estacionaria. 3.2.1.1 Análisis visual Enfatizando en las gráficas presentadas para la descomposición de la serie, se observa que esta no cumple estrictamente los principios de estacionariedad. Por ejemplo, la media no permanece constante y se observa la presencia de una tendencia ascendente. 3.2.1.2 Prueba de Dickey-Fuller # 2. Prueba de raíz unitaria (ADF) prueba_adf &lt;- adf.test(ts_data) print(prueba_adf) ## ## Augmented Dickey-Fuller Test ## ## data: ts_data ## Dickey-Fuller = -2.0596, Lag order = 11, p-value = 0.5531 ## alternative hypothesis: stationary Debido a que el valor p (0.55) es superior al nivel de significancia de 0.05, se sugiere que la serie de tiempo posiblemente contiene una raíz unitaria y, por ende, no es estacionaria. Concluyendo, empleamos la función ndiffs para determinar el número óptimo de diferenciaciones necesario para hacer que la serie de tiempo sea estacionaria. El objetivo es identificar el mínimo número de diferenciaciones requeridas para eliminar tanto la tendencia como la estacionalidad de la serie. # Número de diferenciaciones ndiffs(ts_data) ## [1] 1 3.3 Diferenciación La diferenciación puede ayudar a estabilizar la media de una serie de tiempo al eliminar los cambios en el nivel de una serie de tiempo y, por lo tanto, eliminar (o reducir) la tendencia . Considerando las observaciones previas que indican que la serie temporal de temperatura no es estacionaria, se recurre a la diferenciación con el fin de lograr esta estacionariedad deseada. Antes de aplicar la diferenciación, es importante tener en cuenta el número óptimo de diferenciaciones previamente determinado, que en este caso es uno. Por lo tanto, se procede a diferenciar la serie temporal una sola vez y se genera una nueva descomposición para evaluar nuevamente la estacionalidad y la tendencia en la serie diferenciada. ts_data_dif&lt;- diff(ts_data, lag = 360) descom &lt;- decompose(ts_data_dif) plot(descom) Teniendo en cuenta que en la serie hay estacionalidad y al mismo tiempo tendencia, se realiza una diferenciación con de lag de 360, ya que el periodo estacional se presenta en ese periodo, es decir cada 360 dias. Aunque se presentan fluctuaciones alrededor de un nivel constante a lo largo del tiempo, ya no se percibe una tendencia tan definida de crecimiento. Aunque la estacionalidad ya no es tan marcada, todavía se observa graficamente presente en la serie temporal, y además, comienza a hacerse evidente que la serie ya se puede empezar a considerar estacionaria. 3.3.1 Prueba de Dickey-Fuller para la serie diferenciada # 2. Prueba de raíz unitaria (ADF) prueba &lt;- adf.test(ts_data_dif) ## Warning in adf.test(ts_data_dif): p-value smaller than printed p-value print(prueba) ## ## Augmented Dickey-Fuller Test ## ## data: ts_data_dif ## Dickey-Fuller = -7.151, Lag order = 10, p-value = 0.01 ## alternative hypothesis: stationary Se observa que el valor p es 0.01. El cual es menor a el nivel de significancia de 0.05. Esto sugiere que es improbable que la serie de tiempo tenga una raíz unitaria, lo que indica estacionariedad en la serie diferenciada. "],["holt-winters.html", "4 Holt Winters 4.1 Validación del método Holt-Winters 4.2 Aplicación 4.3 Predicciones 4.4 Evaluación del modelo", " 4 Holt Winters El método de Holt-Winters es una técnica de pronóstico ampliamente utilizada en análisis de series temporales que aborda la complejidad de los datos con componentes de nivel, tendencia y estacionalidad. Al incorporar tres parámetros principales -alfa (α), beta (β) y gamma (γ)-, el modelo de Holt-Winters realiza un triple suavizado exponencial que permite adaptarse de manera dinámica a medida que nuevos datos se van incorporando. El parámetro alfa (α) controla el suavizado del componente de nivel, mientras que el beta (β) influye en el suavizado de la tendencia y el gamma (γ) determina el suavizado de la estacionalidad. Estos parámetros son cruciales para establecer cómo se ponderan y suavizan las diferentes componentes de la serie temporal, permitiendo que el modelo capture eficazmente las variaciones en los datos a lo largo del tiempo. Además de estos parámetros, el método de Holt-Winters también considera la longitud de la estacionalidad (m), que indica la cantidad de períodos en un ciclo estacional completo, mejorando así la capacidad del modelo para modelar y prever patrones estacionales en los datos. Aunque el modelo de Holt-Winters es capaz de modelar la mayoría de las características de una serie temporal, siempre habrá variaciones residuales no sistemáticas o ruido que no pueden ser explicadas por el modelo. Estos residuos son una parte natural de cualquier serie temporal y deben ser evaluados para comprender la precisión y las limitaciones del modelo. 4.1 Validación del método Holt-Winters Para aplicar con éxito el método de Holt-Winters a esta serie de tiempo, es fundamental verificar varios aspectos: Componentes de la serie: Es imprescindible que nuestra serie exhiba al menos dos de las tres componentes principales: nivel, tendencia y estacionalidad. Tras un análisis previo, hemos identificado tanto la tendencia como la estacionalidad en nuestra serie temporal. Patrones estacionales: Hemos confirmado la presencia de estacionalidad en nuestra serie mediante la prueba de Kendall, observamos patrones claros y repetitivos al graficar los datos, particularmente en ciclos anuales y de forma aditiva. Estabilidad y regularidad: Nuestra serie temporal muestra un nivel adecuado de estabilidad y regularidad a lo largo del tiempo, sin exhibir variaciones extremas o fluctuaciones erráticas. Cantidad de datos disponibles: Contamos con una cantidad suficiente de datos para aplicar el método de Holt-Winters. Nuestra serie temporal abarca 1576 registros desde el año 2013 hasta el 2017, con una frecuencia diaria. Datos completos: La serie temporal no presenta valores faltantes; cada uno de los 1576 registros diarios contiene información válida. Estacionariedad: Aunque inicialmente nuestra serie no era estacionaria según el test de Dickey-Fuller, aplicamos una diferenciación una sola vez para estabilizar la media mientras manteníamos la estacionalidad y la tendencia. Teniendo en cuenta estos puntos, nuestra serie de tiempo parece adecuada para aplicar el método de Holt-Winters y realizar un análisis detallado. 4.2 Aplicación Como se mencionó anteriormente, el método de Holt-Winters cuenta con varios parámetros que influyen en su desempeño. Si bien se podria aplicar el método directamente a nuestra serie temporal utilizando la función HoltWinters con los parámetros predeterminados, en nuestro caso optaremos por otra estrategia. Desarrollaremos una función que nos ayude a identificar los valores óptimos de los parámetros, con el objetivo de minimizar el error en nuestras predicciones. Es importante considerar que estos parámetros pueden variar en un rango de 0 a 1. 4.2.1 Optimizar parámetros # La función toma dos argumentos: un vector que contiene los parámetros alfa, beta y gamma, y la serie de tiempo para la cual queremos calcular el error. error_func &lt;- function(params, serie_tiempo) { alpha &lt;- params[1] beta &lt;- params[2] gamma &lt;- params[3] # Ajusta el modelo Holt-Winters hw_model &lt;- HoltWinters(serie_tiempo, alpha = alpha, beta = beta, gamma = gamma) # Calcula el error cuadrático total error &lt;- sum((serie_tiempo - fitted(hw_model))^2) return(error) } # En primer lugar definimos el rango para los parámetros alpha, beta y gamma. lim_inf &lt;- c(0, 0, 0) lim_sup &lt;- c(1, 1, 1) # Se utiliza la función &quot;optim&quot; para encontrar los valores óptimos de los parámetros alpha, beta y gamma, utilizando el método &quot;L-BFGS-B&quot; que es uno de los métodos disponibles en R para la optimización con restricciones. optimizacion &lt;- optim(c(0.5, 0.5, 0.5), error_func, serie_tiempo = ts_data_dif, lower = lim_inf, upper = lim_sup, method = &quot;L-BFGS-B&quot;) # Obtiene los valores óptimos de los parámetros parametros &lt;- optimizacion$par print(parametros) ## [1] 0.5057046 0.0000000 0.8675986 Como podemos evidenciar los valores óptimos son: Alpha = 0.56. Indica que se le da más peso a las observaciones recientes para la estimación del nivel. Beta = 0.00. Indica que no se está considerando la componente de tendencia en el modelo Holt-Winters. Gamma 0.86. Indica que se da un peso significativo a las observaciones recientes para la estimación de la estacionalidad. 4.2.2 Modelo Holt-Winters # Ajustar el modelo HoltWinters con los parámetros definidos anteriormente. hw_model &lt;- HoltWinters(ts_data_dif, alpha = parametros[1], beta = parametros[2], gamma = parametros[3]) plot(hw_model) Como podemos observar, parece que el modelo se ajusta bien a nuestra serie temporal, capturando tanto la tendencia como la estacionalidad. No obstante, más adelante evaluaremos a fondo nuestro modelo. 4.3 Predicciones Utilizando la función forecast, se generan predicciones de nuestro modelo Holt-Winters con niveles de confianza del 80% y del 95%. hw_forecast &lt;- forecast(hw_model,level=c(80,95)) # Visualizar las predicciones plot(hw_forecast, xlim = c(2014, 2018)) lines(hw_forecast$fitted, lty=2, col=&quot;purple&quot;) Como se indica, las líneas negras representan los datos de nuestra serie temporal. Las líneas moradas representan el modelo ajustado. Las líneas azules representan las predicciones para los próximos 360 días o el proximo año. Las áreas grises representan los intervalos de confianza asociados con estas predicciones. Como se puede observar, los intervalos se desbordan, lo que indica una alta incertidumbre en nuestras predicciones. 4.4 Evaluación del modelo La librería forecast también evalúa la calidad de nuestras predicciones al calcular la diferencia entre los valores observados y los valores predichos para cada punto de datos. Estos residuos se incorporan a nuestro modelo de pronóstico como $residuals. Nuestro objetivo es verificar la ausencia de correlaciones entre los errores de pronóstico.Para evaluar esto, utilizamos la función acf para analizar la correlación de los residuos entre puntos separados en el tiempo. Además, la prueba de Ljung-Box puede proporcionar información sobre estas correlaciones. Un valor p mayor que 0.05 indica una probabilidad del 95% de que los residuos sean independientes. Finalmente, es útil examinar el histograma de los residuos para verificar su distribución. Si los residuos están notablemente sesgados, es posible que nuestro modelo esté consistentemente sobrestimando o subestimando en una dirección específica. acf(hw_forecast$residuals, lag.max=360, na.action=na.pass) Box.test(hw_forecast$residuals, type=&quot;Ljung-Box&quot;) ## ## Box-Ljung test ## ## data: hw_forecast$residuals ## X-squared = 60.018, df = 1, p-value = 9.437e-15 hist(hw_forecast$residuals) De acuerdo a los resultados obtenidos mediante el análisis de la función acf, la prueba de Ljung-Box y el histograma, se evidencia la presencia de autocorrelación significativa en los residuos del modelo Holt-Winters. Esto sugiere que el modelo podría no estar capturando completamente la estructura temporal de los datos y podría necesitar ajustes o mejoras adicionales. "],["metodología-box-jenkins-y-modelo-arima.html", "5 Metodología Box-Jenkins y Modelo ARIMA 5.1 Modelo ARIMA 5.2 Metodología Box-Jenkins 5.3 Aplicación", " 5 Metodología Box-Jenkins y Modelo ARIMA La metodología Box-Jenkins es un enfoque sistemático ampliamente utilizado en el análisis de series de tiempo, se aplica a los modelos autorregresivos de media móvil ARMA o a los modelos autorregresivos integrados de media móvil (ARIMA) para encontrar el mejor ajuste de una serie temporal de valores, a fin de que los pronósticos sean más acertados. 5.1 Modelo ARIMA Un modelo ARIMA es una técnica estadística utilizada para analizar y predecir valores futuros en una serie temporal basándose en sus datos históricos. El modelo ARIMA combina tres componentes esenciales: AR (AutoRegressive): Captura la dependencia de los valores actuales de la serie con sus valores pasados. I (Integrated): Involucra la diferenciación de la serie de tiempo para convertirla en estacionaria, eliminando tendencias y variaciones no constantes. MA (Moving Average): Refleja la relación entre los valores actuales de la serie y los errores de predicción pasados. El objetivo principal de utilizar la metodología Box-Jenkins y los modelos ARIMA es encontrar la mejor representación matemática de la serie de tiempo para realizar predicciones precisas y confiables. 5.2 Metodología Box-Jenkins 1.Verificación de estacionariedad y diferenciación: Verificar si la serie de tiempo es estacionaria, lo cual significa que sus propiedades estadísticas, como la media y la varianza, son constantes a lo largo del tiempo.Si la serie es estacional y no es estacionaria, aplicar una diferenciación para hacerla estacionaria (diferenciación estacional, para cierto período, si es necesario). 2.Identificación de p, d, q para el Modelo ARIMA: Ajustar el modelo ARIMA a la serie de tiempo utilizando técnicas de estimación como la máxima verosimilitud. Esto implica encontrar los mejores valores para los parámetros p,d y q que minimicen los errores de predicción. 3.Validación y análisis de residuos: Después de ajustar el modelo, es IMPORTANTE evaluar los residuos (errores de predicción). Los residuos deben comportarse como un ruido blanco, es decir, no deben mostrar patrones ni autocorrelación significativa. 4.Uso del modelo ajustado para realizar predicciones: Una vez validado el modelo, se puede utilizar para predecir valores futuros de la serie de tiempo. Las predicciones se basan en la dinámica interna capturada por el modelo ARIMA, proporcionando estimaciones para períodos futuros. 5.3 Aplicación 1.Verificación de estacionariedad y diferenciación: En análisis anteriores de esta serie de tiempo, se aplicó la diferenciación para convertir la serie en estacionaria, teniendo en cuenta la presencia de estacionalidad. Se realizó la prueba de Dickey-Fuller para confirmar la estacionariedad de la serie después de la diferenciación. De esta manera, se cumple el primer supuesto de la metodología Box-Jenkins, que requiere una serie de tiempo estacionaria antes de proceder con el modelado. 2.Identificación de p, d, q para el Modelo ARIMA: # Ajustar el modelo ARIMA automáticamente fit &lt;- auto.arima(ts_data) # Resumen del modelo ajustado summary(fit) ## Series: ts_data ## ARIMA(3,1,1)(0,1,0)[365] ## ## Coefficients: ## ar1 ar2 ar3 ma1 ## 0.7150 0.0112 -0.0392 -0.9873 ## s.e. 0.0311 0.0371 0.0311 0.0084 ## ## sigma^2 = 4.597: log likelihood = -2397.96 ## AIC=4805.92 AICc=4805.98 BIC=4830.93 ## ## Training set error measures: ## ME RMSE MAE MPE MAPE MASE ## Training set 0.01740133 1.853476 1.214827 -0.2674292 5.197467 0.4752757 ## ACF1 ## Training set 0.002740674 El resultado proporcionado por el resumen anterior muestra lo siguiente: El modelo ajustado se especifica como ARIMA(3,1,1)(0,1,0)[365]. El primer conjunto de números (3,1,1) indica el orden de autoregresión (p), el grado de diferenciación (d), y el orden de la media móvil (q) del componente no estacional del modelo. El segundo conjunto de números (0,1,0) indica lo mismo para el componente estacional. El número entre corchetes [365] indica el período estacional, el cual concuerda con el análisis hecho anteriormente. Los coeficientes estimados para los términos autoregresivos son (ar1, ar2, ar3) y el término de media móvil (ma1) junto con sus errores estándar (s.e.). Estos coeficientes indican cómo los valores pasados de la serie de tiempo afectan a los valores actuales y cómo los errores pasados afectan a los valores actuales. Se observa el error medio (ME), el error cuadrático medio (RMSE), el error absoluto medio (MAE), el error porcentual medio (MPE), el error porcentual absoluto medio (MAPE), el error de desviación media absoluta (MASE) y el primer coeficiente de autocorrelación (ACF1). Según los valores de estos errores el modelo en general, parece que el modelo ARIMA tiene un buen ajuste a los datos históricos de entrenamiento, ya que los errores son relativamente bajos en comparación con las escalas de las variables. 3.Validación y análisis de residuos: # Diagnóstico de los residuos checkresiduals(fit) ## ## Ljung-Box test ## ## data: Residuals from ARIMA(3,1,1)(0,1,0)[365] ## Q* = 360.51, df = 289, p-value = 0.002664 ## ## Model df: 4. Total lags used: 293 Se observa un valor p bajo (p=0.002664), sugiere que hay evidencia de autocorrelación significativa en los residuos del modelo ARIMA, lo que indica que el modelo puede no estar capturando completamente la estructura de autocorrelación en los datos. Esto sugiere que el modelo podría necesitar ajustes adicionales. 5.Uso del modelo para realizar predicciones: # Predicción de los siguientes 360 dias forecasted_values &lt;- forecast(fit, h = 360) # Visualizar las predicciones plot(forecasted_values) En general Se ha observa que el pronóstico para un período completo de 365 días aún muestra o mantiene la estacionalidad y pareciera que se adaptara a la serie. Para obtener un mejor modelo se podria intentar aumentar o variar el orden de los términos autoregresivos, diferenciales o de media móvil en el modelo ARIMA para capturar mejor la estructura de autocorrelación en los datos. Sin embargo, podría existir un riesgo de sobreajustar el modelo. "]]
