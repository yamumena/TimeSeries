--- 
title: "Series de Tiempo Actividad 1"
author: 
        - "Grupo 5:"
        - "Yamuna Mena Ramirez "
        - "Angelica Forero Artunduaga"
        - "Leidy Conde Chavarro"
date: "`r Sys.Date()`"
site: bookdown::bookdown_site
documentclass: book
bibliography: [book.bib, packages.bib]
description: 
link-citations: yes
github-repo: rstudio/bookdown-demo

---

# Selección de la base de datos

Se ha elegido la base de datos **“Daily Climate time series data”**, disponible al público y desarrollada por la Universidad PES. Se Puede acceder a ella a través del siguiente enlace::

https://www.kaggle.com/datasets/sumanthvrao/daily-climate-time-series-data/data.

Esta base de datos contiene registros meteorológicos recopilados en la ciudad de Delhi durante un período de 4 años (de 2013 a 2017). Los atributos incluyen la fecha, temperatura media, humedad, velocidad del viento y presión atmosférica, con más de 1400 registros disponibles.

## Selección de la Variable **Temperatura**

Se elige la serie de tiempo de la temperatura como variable para el proceso de pronóstico. Esta decisión se basa en la relevancia global de la temperatura y su influencia significativa en múltiples áreas.

En el contexto práctico, el pronostico de la temperatura es beneficioso porque:

* El pronostico de la temperatura facilita la anticipación de condiciones climáticas extremas como olas de calor, tormentas y fenómenos como El Niño y La Niña. Esto permite adoptar medidas de protección para garantizar la seguridad de las personas y sus propiedades.

* La temperatura es de gran importancia en sectores como la agricultura, donde afecta los ciclos de cultivo y la productividad. Asimismo, en el ámbito energético, las variaciones de temperatura influyen directamente en la demanda de energía y la eficiencia de fuentes renovables como la solar y eólica. 

* El pronóstico preciso de la temperatura es importante para la economía, ya que permite a las empresas y sectores económicos anticipar y mitigar riesgos financieros asociados con eventos climáticos. Al contar con información anticipada sobre las variaciones de temperatura, las empresas pueden ajustar su producción, inventario y estrategias de mercado, evitando pérdidas económicas por condiciones climáticas adversas.

* La comprensión de la temperatura a lo largo del tiempo contribuye al estudio del clima y el cambio climático. Esto apoya la investigación científica en áreas como meteorología y climatología, proporcionando datos valiosos para entender mejor nuestro entorno y los desafíos que enfrentamos.


```{r librerias, include=FALSE}
library(tidyverse)
#library(dplyr)
library(ggplot2)
library(readxl)
library(tseries)
library(forecast)
library(Kendall)
library(prophet)
```

# Análisis Exploratorio de Datos (EDA): Promedio Móvil, Rezagos y Estacionalidad

En esta sección, proporcionaremos una vista general de los datos utilizando algunas técnicas y pruebas, que incluyen la implementación del promedio móvil, análisis de rezagos y detección de estacionalidad. Estas herramientas ayudan a comprender mejor el comportamiento general de la serie temporal de la temperatura.

## Carga de datos 

Se carga toda la base de datos y se elige solamente la variable temperatura. Con un total de 1.576 registros.

```{r}
# Cargar datos
data1 <- read.csv("C:/Maestria Ciencia de Datos/TStempe/DailyDelhiClimateTrain.csv", sep = ",", dec = ".")
data2 <- read.csv("C:/Maestria Ciencia de Datos/TStempe/DailyDelhiClimateTest.csv", sep = ",", dec = ".")
# Unir datos
data <- rbind(data1, data2)
# Convertir los datos a serie de tiempo
ts_data <-  ts(data,start = c(2013, 1), end = c(2017, 4), frequency=365)
# Escoger solo Temperatura media
ts_data <- ts_data[, "meantemp"]


```

## Resumen general de la serie temporal


```{r}
# Información general
str(ts_data)

# Valores nulos
sum(is.na(ts_data))

```

```{r}
# Resumen estadistico 
summary(ts_data)

```

* A partir de la información proporcionada en el sitio de descarga se sabe que cada registro de la Temperatura media es promediada a partir de múltiples intervalos de 3 horas en un día.

* Hay 1576 registros

* No hay valores nulos

* Hay resgistros desde el 01/01/2013 hasta el 24/04/2017 

* El valor minimo registro es 6 y el maximo es 38.71

## Visualización de la serie temporal

```{r}
plot(ts_data, main = "Temperatura media", xlab = "Año")
```

* A partir de la grafica anterior, se aprecia un patrón periódico anual, con mínimos oscilando entre 5 y 15, y máximos cercanos a 35.

## Media movil

La media movil se aplica a toda la serie para suavizarla y obtener una mejor comprensión de su comportamiento. Este proceso ayuda a resaltar tendencias y a identificar mejor cualquier patrón estacional que pueda estar presente en la serie de temperatura.

```{r message=FALSE, warning=FALSE}
library(zoo)
ventana <- 50
serie_suavizada <- rollmean(ts_data, k = ventana, align = "center", fill = NA)
# Grafica la serie original y suavizada
plot(ts_data, type = "l", col = "blue", lwd = 2, main = "Serie Temporal con Media Móvil",
     xlab = "Tiempo", ylab = "Valor Original")
lines(serie_suavizada, col = "red", lwd = 2)
legend("topright", legend = c("Original", "Suavizada"), col = c("blue", "red"), lwd = 1, cex = 0.6)
```

* En la gráfica anterior, al observar la serie suavizada se ve representada de manera más clara la estacionalidad de esta variable en el tiempo , confirmando la observación inicial sobre su periodicidad anual. Además, se aprecia que los mínimos son mas grandes y los máximos son menos prominentes que la serie original.

* Se eligió una ventana de 50 después de experimentar con diferentes tamaños de ventana, buscando aquella que mostrara de manera más clara el comportamiento de la serie con menos iteraciones o variabilidad de la serie original, con el objetivo de obtener una mejor representación de la serie .


## Rezago

El análisis de rezago es una técnica comúnmente empleada en el análisis de series de tiempo para examinar la correlación entre una serie temporal y sus valores rezagados en el tiempo. Su objetivo principal es identificar patrones temporales, evaluar la dependencia temporal, detectar estacionalidad y facilitar la selección y validación de modelos adecuados.

### Calcular la autocorrelación

Se emplea la función ACF para calcular la autocorrelación con un rezago de 365 días, lo que permite analizar un período completo de un año. En el gráfico, el rezago se representará en el eje x en una escala de 0 a 1. Para interpretarlo en días, se multiplica el valor del rezago por 365. Paralelamente, la autocorrelación se visualiza en el eje y, abarcando un rango de -1 a 1.


```{r}

acf_result <- acf(ts_data, lag.max = 365, main = iconv("Autocorrelación de Temperatura", to = "UTF-8"))
```

* Se observa que a medida de que nos alejamos del primer valor, las correlaciones disminuyen gradualmente hasta que eventualmente se vuelven negativas. El pico más alto negativo se registra en el rezago 196 (equivalente a 195 días atrás), con una correlación de -0.5927, lo que sugiere una reversión de tendencia a medida que los rezagos aumentan. Sin embargo, la correlación vuelve a disminuir hasta el rezago 288 (287 días atrás), donde se observa un nuevo incremento en la correlación. Este patrón sugiere la presencia de un ciclo estacional en los datos de temperatura. Dado que se estan analizando datos de temperatura diaria, esto confirmaria el ciclo estacional anual que se ha venido observando.

* Es importante destacar que las correlaciones positivas indican una tendencia a que las temperaturas altas o bajas persistan en el tiempo. Por ejemplo, si hoy es un día cálido, es probable que mañana también lo sea. En contraste, cuando las correlaciones son negativas, sugieren una reversión en la tendencia. Es decir, si hoy es más cálido de lo normal, es probable que mañana sea más frío de lo normal, y viceversa. En este caso, esta reversión negativa se observa entre el rezago 89 (88 días atrás) y el 279 (278 días atrás).


## Estacionalidad

Hasta el momento, hemos observado ciclos distintivos en la serie de tiempo proporcionada, la cual se enfoca en la temperatura. Tales ciclos son típicos en series temporales asociadas al clima. Para determinar la presencia de estacionalidad en una serie de tiempo, se puede utilizar:

1. Análisis gráfico: Al representar gráficamente la serie de tiempo, es posible identificar patrones recurrentes que sugieren estacionalidad, como la presencia de picos y valles que se repiten en intervalos regulares.

2. Prueba de estacionalidad de Mann-Kendall:  Esta prueba, que es no paramétrica, evalúa la presencia de tendencias monótonas a lo largo del tiempo en la serie de tiempo. Una serie que exhibe estacionalidad a menudo carecerá de una tendencia monótona clara.

3. Análisis de autocorrelación: Al calcular la autocorrelación de la serie de tiempo en diferentes rezagos, se puede determinar si existen autocorrelaciones significativas en los rezagos que están relacionados con la longitud de la estacionalidad supuesta. Como pudimos observar en el análisis de rezago, existen autocorrelaciones significativas en algunos rezagos que podrían indicarnos una estacionalidad anual.Como observamos en el análisis de rezagos, hemos identificado autocorrelaciones significativas en ciertos rezagos. Estas observaciones sugieren la posibilidad de una estacionalidad anual en los datos.

4. Pruebas de estacionalidad de Box-Pierce o Ljung-Box: Estas pruebas se utiliza para evaluar si hay autocorrelaciones significativas en la serie después de eliminar cualquier componente de tendencia y estacionalidad. Si hay autocorrelaciones significativas en los rezagos que corresponden a la estacionalidad, esto puede indicar la presencia de estacionalidad en la serie.

5. Descomposición estacional: Mediante técnicas de descomposición de series de tiempo, es posible separar los componentes de tendencia, estacionalidad y aleatoriedad. Si la componente estacional es notable en comparación con los otros componentes, esto puede indicar la presencia de estacionalidad en la serie.

### Prueba de kendall

```{r}
# Prueba de Kendall
test_kendall <- MannKendall(ts_data)
print(test_kendall)
```

* El resultado de la prueba muestra un valor de tau de 0.146 y un p-valor menor a 0.05. Esto indica que hay evidencia significativa para rechazar la hipótesis nula de ausencia de estacionalidad en la serie de tiempo. Es decir, se sugiere la presencia de una tendencia monotónica en los datos, lo que podría ser indicativo de estacionalidad.

* Retomando el análisis grafico que se ha venido haciendo y de rezago, se puede comprobar visualmente que la serie presenta un comportamiento estacional cada año.

# Análisis Exploratorio de Datos (EDA): Descomposición, Estacionariedad y la Diferenciación

En esta sección, proporcionaremos una vista general de los datos utilizando la evaluaciòn de la descomposición, estacionariedad y la diferenciación.

## Descomposición 

La descomposición de series de tiempo es una técnica esencial en el análisis de datos temporales, permitiendo desglosar una serie en sus componentes principales: tendencia, estacionalidad y ruido. Al aplicar esta técnica a la serie temporal de temperatura, se revela el comportamiento estacional observado previamente, así como la tendencia y la componente de ruido de manera detallada. Esta última representa la variabilidad aleatoria o irregular que no puede atribuirse a la tendencia ni a la estacionalidad previamente identificadas.

```{r}
descomposicion <- decompose(ts_data)

plot(descomposicion)

```

En las graficas anteriores se observa lo siqguiente:

* La tendencia de la serie temporal indica un aumento gradual en las temperaturas a lo largo del tiempo, aunque inicialmente este incremento puede parecer moderado o bastante lento. Sin embargo, en los últimos dos años se ha observado un incremento más pronunciado, sugiriendo que el calentamiento global podría estar acelerándose. y esperando ese posible comportamiento al aumento en proximos años.

* A pesar de que hay una tendencia lenta o moderada hacia el aumento de la temperatura al pasar los años, la estacionalidad parece mantenerse constante.

* En cuanto al componente de ruido, se evidencian fluctuaciones que no pueden ser explicadas por la tendencia ni por la estacionalidad de la serie temporal de temperatura. A pesar de la presencia de estas fluctuaciones no sistemáticas, aún se percibe claramente la presencia de la tendencia y la estacionalidad en la serie de tiempo.

## Estacionariedad

Para que una serie de tiempo sea considerada estacionaria, es importante que tanto la media como la varianza se mantengan constantes a lo largo del tiempo. Es decir, la serie no debe exhibir tendencias a largo plazo ni variaciones sistemáticas en su comportamiento a medida que transcurre el tiempo. Para lograr la estacionariedad, se deben cumplir los siguientes requisitos:

- Media Constante: La media de la serie debe permanecer invariable a lo largo del tiempo, lo que indica la ausencia de tendencias significativas hacia arriba o hacia abajo en la serie.

- Varianza Constante: Asimismo, la varianza de la serie debe mantenerse constante a lo largo del tiempo. Esto implica que la amplitud de las fluctuaciones alrededor de la media no experimenta cambios significativos a medida que progresa el tiempo.

- Autocovarianza Constante: Es fundamental que la covarianza entre dos puntos de datos separados por un cierto intervalo de tiempo permanezca constante. En otras palabras, la relación entre los valores de la serie en diferentes momentos no debe cambiar a medida que avanza el tiempo.

Estos principios son fundamentales para garantizar la estacionariedad de una serie de tiempo, aunque en la práctica se pueden utilizar técnicas adicionales, como la diferenciación, para lograr este objetivo.

### Verificar si la serie de temperatura es estacionaria

Para evaluar la estacionariedad de una serie de tiempo, podemos llevar a cabo varios métodos:

1. Realizar un análisis visual aplicando los principios previamente mencionados para detectar si la serie presenta una media y varianza constantes a lo largo del tiempo.

2. Aplicar la prueba de Dickey-Fuller, la cual busca determinar si una serie de tiempo posee una raíz unitaria. La presencia de esta raíz implica una tendencia determinística en la serie, lo cual contradice los principios de estacionariedad mencionados anteriormente. La hipótesis nula de esta prueba es que la serie de tiempo tiene una raíz unitaria, lo que indica no estacionariedad. Si el valor p asociado con la prueba es menor que un nivel de significancia predefinido (generalmente 0.05), se rechaza la hipótesis nula y se concluye que la serie de tiempo es estacionaria.

#### Análisis visual 

* Enfatizando en las gráficas presentadas para la descomposición de la serie, se observa que esta no cumple estrictamente los principios de estacionariedad. Por ejemplo, la media no permanece constante y se observa la presencia de una tendencia ascendente.

#### Prueba de Dickey-Fuller

```{r}
# 2. Prueba de raíz unitaria (ADF)

prueba_adf <- adf.test(ts_data)

print(prueba_adf)
```

* Debido a que el valor p (0.55) es superior al nivel de significancia de 0.05, se sugiere que la serie de tiempo posiblemente contiene una raíz unitaria y, por ende, no es estacionaria.

Concluyendo, empleamos la función ndiffs para determinar el número óptimo de diferenciaciones necesario para hacer que la serie de tiempo sea estacionaria. El objetivo es identificar el mínimo número de diferenciaciones requeridas para eliminar tanto la tendencia como la estacionalidad de la serie.

```{r}
# Número de diferenciaciones
ndiffs(ts_data)
```

## Diferenciación

La diferenciación puede ayudar a estabilizar la media de una serie de tiempo al eliminar los cambios en el nivel de una serie de tiempo y, por lo tanto, eliminar (o reducir) la tendencia . Considerando las observaciones previas que indican que la serie temporal de temperatura no es estacionaria, se recurre a la diferenciación con el fin de lograr esta estacionariedad deseada.

Antes de aplicar la diferenciación, es importante tener en cuenta el número óptimo de diferenciaciones previamente determinado, que en este caso es uno. Por lo tanto, se procede a diferenciar la serie temporal una sola vez y se genera una nueva descomposición para evaluar nuevamente la estacionalidad y la tendencia en la serie diferenciada.

```{r}
ts_data_dif<- diff(ts_data, lag = 360)
descom <- decompose(ts_data_dif)
plot(descom)
```

* Teniendo en cuenta que en la serie hay estacionalidad y al mismo tiempo  tendencia, se realiza una diferenciación con de lag de 360, ya que el periodo estacional se presenta en ese periodo, es decir cada 360 dias.

* Aunque se presentan fluctuaciones alrededor de un nivel constante a lo largo del tiempo, ya no se percibe una tendencia tan definida de crecimiento.

* Aunque la estacionalidad ya no es tan marcada, todavía se observa graficamente presente en la serie temporal, y además, comienza a hacerse evidente que la serie ya se puede empezar a considerar estacionaria. 

### Prueba de Dickey-Fuller para la serie diferenciada

```{r}
# 2. Prueba de raíz unitaria (ADF)

prueba <- adf.test(ts_data_dif)

print(prueba)
```

* Se observa que el valor p es 0.01. El cual es menor a el nivel de significancia de 0.05. Esto sugiere que es improbable que la serie de tiempo tenga una raíz unitaria, lo que indica estacionariedad en la serie diferenciada.

# Holt Winters

El método de Holt-Winters es una técnica de pronóstico ampliamente utilizada en análisis de series temporales que aborda la complejidad de los datos con componentes de nivel, tendencia y estacionalidad. Al incorporar tres parámetros principales -alfa (α), beta (β) y gamma (γ)-, el modelo de Holt-Winters realiza un triple suavizado exponencial que permite adaptarse de manera dinámica a medida que nuevos datos se van incorporando.

El parámetro alfa (α) controla el suavizado del componente de nivel, mientras que el beta (β) influye en el suavizado de la tendencia y el gamma (γ) determina el suavizado de la estacionalidad. Estos parámetros son cruciales para establecer cómo se ponderan y suavizan las diferentes componentes de la serie temporal, permitiendo que el modelo capture eficazmente las variaciones en los datos a lo largo del tiempo.

Además de estos parámetros, el método de Holt-Winters también considera la longitud de la estacionalidad (m), que indica la cantidad de períodos en un ciclo estacional completo, mejorando así la capacidad del modelo para modelar y prever patrones estacionales en los datos.

Aunque el modelo de Holt-Winters es capaz de modelar la mayoría de las características de una serie temporal, siempre habrá variaciones residuales no sistemáticas o ruido que no pueden ser explicadas por el modelo. Estos residuos son una parte natural de cualquier serie temporal y deben ser evaluados para comprender la precisión y las limitaciones del modelo.

## Validación del método Holt-Winters

Para aplicar con éxito el método de Holt-Winters a esta serie de tiempo, es fundamental verificar varios aspectos:

1. Componentes de la serie: Es imprescindible que nuestra serie exhiba al menos dos de las tres componentes principales: nivel, tendencia y estacionalidad. Tras un análisis previo, hemos identificado tanto la tendencia como la estacionalidad en nuestra serie temporal.

2. Patrones estacionales: Hemos confirmado la presencia de estacionalidad en nuestra serie mediante la prueba de Kendall, observamos patrones claros y repetitivos al graficar los datos, particularmente en ciclos anuales y de forma aditiva.

3. Estabilidad y regularidad: Nuestra serie temporal muestra un nivel adecuado de estabilidad y regularidad a lo largo del tiempo, sin exhibir variaciones extremas o fluctuaciones erráticas.

4. Cantidad de datos disponibles: Contamos con una cantidad suficiente de datos para aplicar el método de Holt-Winters. Nuestra serie temporal abarca 1576 registros desde el año 2013 hasta el 2017, con una frecuencia diaria.

5. Datos completos: La serie temporal no presenta valores faltantes; cada uno de los 1576 registros diarios contiene información válida.

6. Estacionariedad: Aunque inicialmente nuestra serie no era estacionaria según el test de Dickey-Fuller, aplicamos una diferenciación una sola vez para estabilizar la media mientras manteníamos la estacionalidad y la tendencia.

Teniendo en cuenta estos puntos, nuestra serie de tiempo parece adecuada para aplicar el método de Holt-Winters y realizar un análisis detallado.

## Aplicación

Como se mencionó anteriormente, el método de Holt-Winters cuenta con varios parámetros que influyen en su desempeño. Si bien se podria aplicar el método directamente a nuestra serie temporal utilizando la función HoltWinters con los parámetros predeterminados, en nuestro caso optaremos por otra estrategia. Desarrollaremos una función que nos ayude a identificar los valores óptimos de los parámetros, con el objetivo de minimizar el error en nuestras predicciones. Es importante considerar que estos parámetros pueden variar en un rango de 0 a 1.

### Optimizar parámetros

```{r}
# La función toma dos argumentos: un vector que contiene los parámetros alfa, beta y gamma, y la serie de tiempo para la cual queremos calcular el error.

error_func <- function(params, serie_tiempo) {
  alpha <- params[1]
  beta <- params[2]
  gamma <- params[3]
  
  # Ajusta el modelo Holt-Winters
  hw_model <- HoltWinters(serie_tiempo, alpha = alpha, beta = beta, gamma = gamma)
  
  # Calcula el error cuadrático total
  error <- sum((serie_tiempo - fitted(hw_model))^2)
  
  return(error)
}
```

```{r}
# En primer lugar definimos el rango para los parámetros alpha, beta y gamma.
lim_inf <- c(0, 0, 0)
lim_sup <- c(1, 1, 1)
```

```{r}
# Se utiliza la función "optim" para encontrar los valores óptimos de los parámetros alpha, beta y gamma, utilizando el método "L-BFGS-B" que es uno de los métodos disponibles en R para la optimización con restricciones.

optimizacion <- optim(c(0.5, 0.5, 0.5), error_func, serie_tiempo = ts_data_dif, lower = lim_inf, upper = lim_sup,  method = "L-BFGS-B")

```

```{r}

# Obtiene los valores óptimos de los parámetros

parametros <- optimizacion$par
print(parametros)

```

Como podemos evidenciar los valores óptimos son:

1. Alpha = 0.56. Indica que se le da más peso a las observaciones recientes para la estimación del nivel.

2. Beta = 0.00. Indica que no se está considerando la componente de tendencia en el modelo Holt-Winters.

3. Gamma 0.86. Indica que se da un peso significativo a las observaciones recientes para la estimación de la estacionalidad.

### Modelo Holt-Winters

```{r}

# Ajustar el modelo HoltWinters con los parámetros definidos anteriormente.

hw_model <- HoltWinters(ts_data_dif, alpha = parametros[1], beta = parametros[2], gamma = parametros[3])

plot(hw_model)

```

* Como podemos observar, parece que el modelo se ajusta bien a nuestra serie temporal, capturando tanto la tendencia como la estacionalidad. No obstante, más adelante evaluaremos a fondo nuestro modelo.

## Predicciones

Utilizando la función forecast, se generan predicciones de nuestro modelo Holt-Winters con niveles de confianza del 80% y del 95%.

```{r}

hw_forecast <- forecast(hw_model,level=c(80,95))

# Visualizar las predicciones

plot(hw_forecast, xlim = c(2014, 2018))
lines(hw_forecast$fitted, lty=2, col="purple")
```

Como se indica, las líneas negras representan los datos de nuestra serie temporal. Las líneas moradas representan el modelo ajustado. Las líneas azules representan las predicciones para los próximos 360 días o el proximo año. Las áreas grises representan los intervalos de confianza asociados con estas predicciones. Como se puede observar, los intervalos se desbordan, lo que indica una alta incertidumbre en nuestras predicciones.

## Evaluación del modelo

La librería forecast también evalúa la calidad de nuestras predicciones al calcular la diferencia entre los valores observados y los valores predichos para cada punto de datos. Estos residuos se incorporan a nuestro modelo de pronóstico como $residuals. Nuestro objetivo es verificar la ausencia de correlaciones entre los errores de pronóstico.Para evaluar esto, utilizamos la función acf para analizar la correlación de los residuos entre puntos separados en el tiempo.

Además, la prueba de Ljung-Box puede proporcionar información sobre estas correlaciones. Un valor p mayor que 0.05 indica una probabilidad del 95% de que los residuos sean independientes.

Finalmente, es útil examinar el histograma de los residuos para verificar su distribución. Si los residuos están notablemente sesgados, es posible que nuestro modelo esté consistentemente sobrestimando o subestimando en una dirección específica.

```{r}

acf(hw_forecast$residuals, lag.max=360, na.action=na.pass)
Box.test(hw_forecast$residuals,  type="Ljung-Box")
hist(hw_forecast$residuals)

```


* De acuerdo a los resultados obtenidos mediante el análisis de la función acf, la prueba de Ljung-Box y el histograma, se evidencia la presencia de autocorrelación significativa en los residuos del modelo Holt-Winters. Esto sugiere que el modelo podría no estar capturando completamente la estructura temporal de los datos y podría necesitar ajustes o mejoras adicionales.

# Metodología Box-Jenkins y Modelo ARIMA

La metodología Box-Jenkins es un enfoque sistemático ampliamente utilizado en el análisis de series de tiempo, se aplica a los modelos autorregresivos de media móvil ARMA o a los modelos autorregresivos integrados de media móvil (ARIMA) para encontrar el mejor ajuste de una serie temporal de valores, a fin de que los pronósticos sean más acertados.

## Modelo ARIMA

Un modelo ARIMA es una técnica estadística utilizada para analizar y predecir valores futuros en una serie temporal basándose en sus datos históricos.

El modelo ARIMA combina tres componentes esenciales:

* AR (AutoRegressive): Captura la dependencia de los valores actuales de la serie con sus valores pasados.

* I (Integrated): Involucra la diferenciación de la serie de tiempo para convertirla en estacionaria, eliminando tendencias y variaciones no constantes.

* MA (Moving Average): Refleja la relación entre los valores actuales de la serie y los errores de predicción pasados.

El objetivo principal de utilizar la metodología Box-Jenkins y los modelos ARIMA es encontrar la mejor representación matemática de la serie de tiempo para realizar predicciones precisas y confiables.

## Metodología Box-Jenkins

**1.**Verificación de estacionariedad y diferenciación:

Verificar si la serie de tiempo es estacionaria, lo cual significa que sus propiedades estadísticas, como la media y la varianza, son constantes a lo largo del tiempo.Si la serie es estacional y no es estacionaria, aplicar una diferenciación para hacerla estacionaria (diferenciación estacional, para cierto período, si es necesario). 

**2.**Identificación de p, d, q para el Modelo ARIMA:

Ajustar el modelo ARIMA a la serie de tiempo utilizando técnicas de estimación como la máxima verosimilitud. Esto implica encontrar los mejores valores para los parámetros **p**,**d** y **q** que minimicen los errores de predicción.

**3.**Validación y análisis de residuos:

Después de ajustar el modelo, es IMPORTANTE evaluar los residuos (errores de predicción). Los residuos deben comportarse como un ruido blanco, es decir, no deben mostrar patrones ni autocorrelación significativa.

**4.**Uso del modelo ajustado para realizar predicciones:

Una vez validado el modelo, se puede utilizar para predecir valores futuros de la serie de tiempo. Las predicciones se basan en la dinámica interna capturada por el modelo ARIMA, proporcionando estimaciones para períodos futuros.

## Aplicación

Verificación de estacionariedad y diferenciación:

En análisis anteriores de esta serie de tiempo, se aplicó la diferenciación para convertir la serie en estacionaria, teniendo en cuenta la presencia de estacionalidad. Se realizó la prueba de Dickey-Fuller para confirmar la estacionariedad de la serie después de la diferenciación. De esta manera, se cumple el primer supuesto de la metodología Box-Jenkins, que requiere una serie de tiempo estacionaria antes de proceder con el modelado.

## Identificación de p, d, q para el Modelo ARIMA:

```{r}

# Ajustar el modelo ARIMA automáticamente
fit <- auto.arima(ts_data)

# Resumen del modelo ajustado
summary(fit)
```

El resultado proporcionado por el resumen anterior muestra lo siguiente:

* El modelo ajustado se especifica como ARIMA(3,1,1)(0,1,0)[365]. El primer conjunto de números (3,1,1) indica el orden de autoregresión (p), el grado de diferenciación (d), y el orden de la media móvil (q) del componente no estacional del modelo.

* El segundo conjunto de números (0,1,0) indica lo mismo para el componente estacional.

* El número entre corchetes [365] indica el período estacional, el cual concuerda con el análisis hecho anteriormente.

* Los coeficientes estimados para los términos autoregresivos son (ar1, ar2, ar3) y el término de media móvil (ma1) junto con sus errores estándar (s.e.). Estos coeficientes indican cómo los valores pasados de la serie de tiempo afectan a los valores actuales y cómo los errores pasados afectan a los valores actuales.

* Se observa el error medio (ME), el error cuadrático medio (RMSE), el error absoluto medio (MAE), el error porcentual medio (MPE), el error porcentual absoluto medio (MAPE), el error de desviación media absoluta (MASE) y el primer coeficiente de autocorrelación (ACF1). Según los valores de estos errores el modelo en general, parece que el modelo ARIMA tiene un buen ajuste a los datos históricos de entrenamiento, ya que los errores son relativamente bajos en comparación con las escalas de las variables. 

## Evaluación del modelo:

```{r}
# Diagnóstico de los residuos
checkresiduals(fit)
```

* Se observa un valor p bajo (p=0.002664), sugiere que hay evidencia de autocorrelación significativa en los residuos del modelo ARIMA, lo que indica que el modelo puede no estar capturando completamente la estructura de autocorrelación en los datos. Esto sugiere que el modelo podría necesitar ajustes adicionales.

## Uso del modelo para realizar predicciones:

```{r}
# Predicción de los siguientes 360 dias
forecasted_values <- forecast(fit, h = 360)

# Visualizar las predicciones
plot(forecasted_values)
```

* En general Se ha observa que el pronóstico para un período completo de 365 días aún muestra o mantiene la estacionalidad y pareciera que se adaptara a la serie.

* Para obtener un mejor modelo se podria intentar aumentar o variar el orden de los términos autoregresivos, diferenciales o de media móvil en el modelo ARIMA para capturar mejor la estructura de autocorrelación en los datos. Sin embargo, podría existir un riesgo de sobreajustar el modelo.


# Aplicación algoritmo Facebook´s Prophet

Este modelo fue introducido por Facebook (S. J. Taylor & Letham, 2018), originalmente para pronosticar datos diarios con estacionalidad semanal y anual, además de efectos de vacaciones. Más tarde se amplió para cubrir más tipos de datos estacionales. Funciona mejor con series temporales que tienen una fuerte estacionalidad y varias temporadas de datos históricos.

Prophet puede considerarse un modelo de regresión no lineal de la forma:

yt=g(t)+s(t)+h(t)+ϵt,

donde g(t) denota la tendencia lineal, s(t) los comportamientos estacionales, h(t) captura los efectos de las vacaciones y ϵt es el ruido blanco.

Consideraciones:

* Los nodos (o puntos de cambio) para la tendencia lineal por tramos se seleccionan automáticamente si no se especifican explícitamente. Opcionalmente, se puede usar una función logística para establecer un límite superior en la tendencia.

* El componente estacional consiste en términos de Fourier de los períodos relevantes. De forma predeterminada, el orden 10 se usa para la estacionalidad anual y el orden 3 se usa para la estacionalidad semanal.

* Los efectos de vacaciones se agregan como variables ficticias simples.

* El modelo se estima utilizando un enfoque bayesiano para permitir la selección automática de los puntos de cambio y otras características del modelo.

## Aplicación del modelo Prophet

En el análisis de las secciones anteriores se evidenció que la serie temporal de la temperatura exhibe una marcada estacionalidad anual. Por lo tanto, el modelo Prophet es adecuado para aplicarse a esta serie.

Se ajusta el modelo Prophet,  se crea un DataFrame para los futuros valores a predecir (por ejemplo, próximos 365 días), y se realizan las predicciones y visualización de los componentes del modelo (tendencia, estacionalidad, etc.)


Seleccionar y renombrar las columnas necesarias para Prophet

```{r}
df <- data %>% 
  select(date, meantemp) %>% 
  rename(ds = date, y = meantemp)


```


Convertir la columna de fecha al formato Date


```{r}
df$ds <- as.Date(df$ds, format = "%Y-%m-%d")
```

```{r}
modeli <- prophet(df)
future <- make_future_dataframe(modeli, periods = 365)
forecast <- predict(modeli, future)
plot(modeli, forecast)
prophet_plot_components(modeli, forecast)
```

## Análisis de los Resultados con Facebook's Prophet

* Gráfico de Predicciones

En el primer gráfico, se observa la predicción de la temperatura media a lo largo del tiempo. la Línea Azul Oscura representa la predicción de la temperatura media, las áreas Sombreadas indican los intervalos de confianza del 80% y 95%. La predicción muestra que se conserva la estacionalidad anual ya que se prresenta el mismo patrón, se observa que los intervalos de son amplios y con mayor incertidumbre en los maximos, esto muestra una coherencia con las dispersion de los datos pasados en esta temporada.

* Gráfico de Tendencia:

Eje Y (trend): Representa la tendencia general de la temperatura.
Eje X (ds): Representa las fechas.

La tendencia es relativamente constante entre 2014 y 2015. En 2016, se observa un aumento drástico, rompiendo la tendencia anterior. A partir de 2016, la tendencia intenta estabilizarse, pero sigue mostrando un ligero aumento. En general, la tendencia es levemente ascendente, lo que sugiere un aumento gradual en la temperatura media a lo largo del tiempo.

* Gráfico Semanal Estacional:

Eje Y (weekly): Representa la variación estacional semanal.
Eje X (Day of week): Representa los días de la semana.

El gráfico muestra una clara estacionalidad semanal, con un aumento en la temperatura que comienza el domingo y alcanza su punto máximo el miércoles. Posteriormente, la temperatura desciende hasta llegar a su punto mínimo el sábado. Este patrón sugiere una variación predecible en la temperatura a lo largo de la semana, ademas que los dias miercoles de cada semana presentan mayores temperaturas y los sabados y los domingos menores.

* Gráfico Anual Estacional:

Eje Y (yearly): Representa la variación estacional anual.
Eje X (Day of year): Representa los días del año.

En este gráfico se observa claramente el patrón estacional anual identificado en análisis anteriores. La temperatura comienza a aumentar desde enero, alcanzando su punto máximo en junio. Posteriormente, la temperatura desciende hasta llegar nuevamente a su punto mínimo en enero del siguiente año, momento en el que vuelve a iniciar el ciclo ascendente. Este patrón sugiere que junio es el mes con las temperaturas más altas, mientras que enero presenta las temperaturas más bajas.

## Justificación para la Variable como una Regresión y Evaluación


```{r}
# Instalar y cargar librerías adicionales si es necesario
library(forecast)

# Diagnóstico de los residuos
residuals <- forecast$yhat - df$y

# Ver los primeros registros de los residuos
head(residuals)

# Realizar pruebas de autocorrelación para los residuos
acf(residuals, main="ACF de los Residuos")

# Prueba de Ljung-Box para verificar la autocorrelación de los residuos
Box.test(residuals, type = "Ljung-Box")

# Gráfico de los residuos
plot(residuals, type='l', main='Residuos del Modelo Prophet', ylab='Residuos', xlab='Tiempo')
abline(h=0, col='red')

```

Para justificar el uso de la serie temporal de temperatura media como una regresión, se consideran varios aspectos basados en el análisis de residuos y las pruebas estadísticas realizadas. Los resultados obtenidos indican si el modelo Prophet es adecuado y si la variable puede ser tratada como una regresión.


* Autocorrelación de los Residuos (ACF):

El gráfico de la ACF de los residuos muestra que existe una fuerte autocorrelación en los residuos a lo largo de múltiples rezagos. Esto indica que el modelo Prophet no ha capturado completamente la estructura temporal de los datos y que los residuos no se comportan como ruido blanco.

* Gráfico de Residuos:

El gráfico de residuos del modelo Prophet muestra un patrón, lo que podria sugerir que los residuos no son estacionarios y tienen una tendencia.

Prueba de Ljung-Box:

El resultado de la prueba de Ljung-Box es significativo (p-valor < 2.2e-16), lo que indica que hay autocorrelación significativa en los residuos. Esto confirma que los residuos del modelo no son independientes y tienen una estructura temporal no capturada por el modelo.

* Comparación de Modelos

Resulta útil contrastar el rendimiento del modelo Prophet con los enfoques previos, tales como ARIMA y Holt-Winters, que hemos examinado anteriormente. Todos los modelos muestran un p-value notablemente bajo, menor que el nivel de significancia establecido, ademas el analisis de ACF indica la presencia de autocorrelación en los residuos. Estas observaciones sugieren que ninguno de los modelos se ajusta adecuadamente a los datos, y por lo tanto, podrían beneficiarse de mejoras adicionales.

* Viabilidad del Modelo Prophet

A pesar de las limitaciones observadas en el análisis de residuos, el modelo Prophet puede seguir siendo útil debido a su capacidad para descomponer la serie temporal en componentes interpretables como tendencia y estacionalidad. Sin embargo, los resultados indican que se requieren ajustes adicionales o el uso de modelos complementarios para mejorar la precisión de las predicciones, bajo estas circunstancias la serie podria no ajustarse totalmente a una regresión.


# Configuración y Entrenamiento de Redes Neuronales Elman y Jordan Red Neuronal Elman

# Redes Neuronales Recurrentes

Una Red Neuronal Recurrente (RNN, por sus siglas en inglés) es un tipo de red neuronal artificial diseñada para procesar secuencias de datos. A diferencia de las redes neuronales tradicionales que consideran las entradas de forma independiente, las RNNs tienen una estructura interna que les permite mantener una "memoria" de entradas anteriores. Esto se logra a través de conexiones recurrentes que retroalimentan la información de las capas ocultas a sí mismas, lo que les permite capturar dependencias temporales en los datos.

Estas redes están compuestos por:

- Capa de entrada: Recibe la secuencia de datos.
- Capa Oculta: Procesa la información actual y la retroalimenta para influir en el procesamiento futuro.
- Capa de Salida: Genera la salida de la red, que puede ser una predicción o una clasificación.

Cada nodo en la capa oculta no solo recibe la entrada actual sino también la salida de la capa oculta del paso anterior. Esto permite que la red mantenga información sobre lo que ha visto anteriormente y ajuste su salida en consecuencia. Las RNNs son particularmente útiles para el pronóstico de series temporales debido a su capacidad para manejar datos secuenciales y capturar patrones temporales.

En este sentido se trabajará con 2 modelos: Modelo Elman y Modelo Jordan.

## Modelo Elman y Modelo Jordan

```{r librerias1, include=FALSE}
library(tidyverse)
library(ggplot2)
library(readxl)
library(tseries)
library(forecast)
library(MLmetrics)
library(prophet)
library(RSNNS)
library(quantmod)
options(scipen = 100, digits = 2)
```

### Normalización de datos

La normalización se realiza para que las entradas de la red neuronal estén en un rango común, generalmente entre 0 y 1. Esto ayuda a mejorar el rendimiento y la estabilidad del entrenamiento del modelo.


```{r}

data_normalizada <- (ts_data-min(ts_data))/(max(ts_data)-min(ts_data))  
plot(data_normalizada)

```
### División entre conjunto de entrenamiento y prueba
Se divide el conjunto de datos en un 80% para entrenamiento y un 20% para test.


```{r}
# Crear conjuntos de entrenamiento y prueba
tamano_total <- length(data_normalizada)
set.seed(123)
tamano_train <- round(tamano_total*0.80, digits = 0)
train_data <-0:(tamano_train-1)
test_data <- (tamano_train):tamano_total
```

### Ventaneo

Se crea un conjunto de datos con un n columnas donde cada columna representa un valor futuro de la serie temporal, utilizando una variable de tipo zoo, equivalente al período de retardo de la serie.Dado que esta serie temporal es diaria, y se toma desde enero de 2013 a marzo de 2017, se construyen 30 columnas que corresponden a 30 días. Esta técnica es conocida como "ventaneo".

Primero se convierte la  variable en tipo zoo y se crea una función que construya estas columnas y un dataframe vacío donde estarán todas las columnas.

```{r}

y<-as.zoo(data_normalizada)
crear_columna <- function(data, n){
  Lag(data,k=n)
}

datalogN <-  list()

```

Se aplica la función 30 veces y se combinan los resultados.

```{r}

for (n in 1:30) {
  datalogN[[n]] <- crear_columna(y, n)
}

datalogN <- do.call(cbind, datalogN)
colnames(datalogN) <- paste0("x", 1:30)
datalogN<-cbind(y,datalogN)
```

Cuando se tiene el conjunto de datos se eliminan los NA desplazando la serie.

```{r}
datalogN <- datalogN[-(1:30),]
```

### Inputs y Outputs

Se definen los valores de entrada y salida de la red neuronal.

```{r}
inputs <- datalogN[,2:31]
outputs <- datalogN[,1]
```

## Modelo Elman

Es una Red neuronal recurrente simple que incorpora una capa de contexto para almacenar y utilizar la información de los estados anteriores de la red, permitiendo así que el modelo tenga una "memoria" a corto plazo.

Estas redes están compuestos por:

- Capa de entrada: Recibe la secuencia de datos.
- Capa Oculta: Realiza las transformaciones no lineales. Se retroalimenta a sí misma a través de la capa de contexto.
- Capa de Contexto: Almacena los valores anteriores de la capa oculta y los usa como entradas adicionales en el siguiente paso de tiempo.
- Capa de Salida: Genera la salida de la red, que puede ser una predicción o una clasificación.

Se configura la red de Elman explorando diversas combinaciones de neuronas en las capas ocultas y número máximo de iteraciones, y aunque apenas se ha explorado el ritmo de aprendizaje, se busca ajustar la curva de predicción al modelo de la serie de la mejor manera posible. se ha establecido una semilla para asegurar la reproducibilidad de los resultados al crear nuestra red neuronal.

```{r}

set.seed(123)
fit<-elman(inputs[train_data],outputs[train_data],size=c(12,5),learnFuncParams=c(0.1),maxit=50000)

```

* Se nota la evolución del error de red con el número de iteraciones para los parámetros expuestos. 


```{r}
plotIterativeError(fit, main = "Error Iterativo")
```

* Se observa que el error converge rápidamente a 0.

Luego se procede a realizar la predicción utilizando los datos restantes de la serie, es decir, los datos seleccionados para el conjunto de prueba. Una vez entrenado el modelo, se evalua y visualiza gráficamente para analizar su ajuste.

```{r}
y <- as.vector(outputs[-test_data])
plot(y,type="l")
pred <- predict(fit, inputs[-test_data])
lines(pred,col = "red")
```

El modelo tiene un ajuste satisfactorio con los parámetros elegidos, ya que la curva del modelo sigue de cerca la curva de la serie predicha. Ahora, aprovechando el efecto de memoria, se avanza la serie temporal al menos en un valor con alta precisión. Para lograr esto, se reintroducen los datos de entrenamiento.

```{r}
predictions <- predict(fit,inputs[-train_data])

```

* Se Desnormalizan los datos.

```{r}
predicciones <- predictions*(max(ts_data)-min(ts_data))+min(ts_data)

```

* Representación de los valores predichos.


```{r}
x <- 1:(tamano_total+length(predicciones))
y <- c(as.vector(ts_data),predicciones)
plot(x[1:tamano_total], y[1:tamano_total],col = "blue", type="l")
lines( x[(tamano_total):length(x)], y[(tamano_total):length(x)], col="red")
```

### Métricas para el modelo

```{r}
data2 <- data[1546:1576,]
data_real <- ts(data2$meantemp, start = c(2017,84), end = c(2017, 113), frequency = 365)
prediccionesc <- predicciones[1:30]
predicciones_ts <- ts(prediccionesc, start = c(2017,84), end = c(2017, 113), frequency = 365)
```

### Desempeño de la red de Elman

```{r}
accuracy(predicciones_ts,data_real)
```

## Modelo Jordan

La principal diferencia entre el modelo Jordan y el modelo Elman radica en cómo se realiza la retroalimentación de la capa oculta:

- Capa de Entrada: Recibe la secuencia de datos de entrada, similar al modelo Elman.
- Capa Oculta: Realiza transformaciones no lineales sobre los datos de entrada. En el modelo Jordan, la capa oculta no se retroalimenta a sí misma directamente como en el modelo Elman.
- Capa de Contexto: En el modelo Jordan, la retroalimentación se realiza desde la capa de salida hacia la capa de contexto. La capa de contexto almacena y utiliza la salida generada en el paso de tiempo anterior como entrada adicional en el siguiente paso de tiempo.
- Capa de Salida: Genera la salida final de la red neuronal, que puede ser una predicción en modelos de series temporales o una clasificación en modelos de aprendizaje supervisado.
En el modelo Jordan, la salida también se utiliza para retroalimentar la capa de contexto.

Construcción del modelo

```{r}
set.seed(42)
fit_jordan <-jordan(inputs[train_data],outputs[train_data],size=12,learnFuncParams=c(0.1),
             maxit=50000)

plotIterativeError(fit_jordan, main = "Error Iterativo")
```



```{r}
# Comparación
yy <- as.vector(outputs[-test_data])
plot(yy,type="l")
predd <- predict(fit_jordan, inputs[-test_data])
lines(predd,col = "red")

```


```{r}
# Predicciones
predictions <- predict(fit_jordan,inputs[-train_data])
predicciones_jordan <- predictions*(max(ts_data)-min(ts_data))+min(ts_data)

```


```{r}
# Predicciones
x <- 1:(tamano_total+length(predicciones_jordan))
y <- c(as.vector(ts_data),predicciones_jordan)
plot(x[1:tamano_total], y[1:tamano_total],col = "blue", type="l")
lines( x[(tamano_total):length(x)], y[(tamano_total):length(x)], col="red")
```


Métricas para el modelo Jordan

```{r}

prediccionesc_jordan <- predicciones_jordan[1:30]
prediccionesjordan_ts <- ts(prediccionesc_jordan, start = c(2017,84), end = c(2017, 113), frequency = 365)
```

### Desempeño de la red de Jordan

```{r}
accuracy(prediccionesjordan_ts,data_real)
```
### Análisis de resultados

Al comparar el desempeño entre las redes de Elman y Jordan aplicadas a la serie de tiempo, la red de Elman mostró un rendimiento significativamente superior. Las métricas de error, incluyendo RMSE, MAE y MAPE, fueron considerablemente más bajas en Elman, indicando predicciones más precisas y cercanas a los valores reales de la stemperaturras. Además, la red de Elman presentó menor autocorrelación en los errores (ACF1) y un menor valor de Theil's U, sugiriendo una mejor capacidad predictiva y una mayor estabilidad en comparación con Jordan. 

